{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict bad LS moves\n",
    "\n",
    "This notebook develops simple models for predicting bad local search moves. Particularly, given nodes $U$ and $V$ in routes $R_U$ and $R_V$, it predicts whether each LS operator we currently have is likely to produce an improving solution if the operator were applied to these node pairs $U$ and $V$.\n",
    "\n",
    "The motivation is that evaluating a full operator move is typically somewhat slow, whereas a fast and reasonably accurate prediction method can completely avoid such evaluations.\n",
    "\n",
    "TODO:\n",
    "- New data on all U/V combinations, not just neighbourhood restricted.\n",
    "- Penalties? For feasibility?\n",
    "- Learning rate and validation stuff?\n",
    "- Better feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Python\\Euro-NeurIPS-2022\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "from glob import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/raw/\")\n",
    "INST_PATH = Path(\"instances/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "These can be used to quickly parse the raw results for a single instance into something that contains the same data, but in a more workable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Route:\n",
    "    clients: list[int]\n",
    "    load: int\n",
    "    tw: int\n",
    "\n",
    "    def index(self, client: int) -> int:\n",
    "        return self.clients.index(client)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> int:\n",
    "        return self.clients[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clients)\n",
    "\n",
    "@dataclass\n",
    "class Record:\n",
    "    op: int\n",
    "    U: int\n",
    "    V: int\n",
    "    delta: int\n",
    "    Ru: Route\n",
    "    Rv: Route\n",
    "\n",
    "def parse_file(file: str) -> list[Record]:\n",
    "    def parse_record(record: list[str]) -> Record:\n",
    "        op = int(record[0].strip())\n",
    "        U, V, delta = map(int, record[1].strip().split(\" \"))\n",
    "        _, *Ru = map(int, re.findall('[0-9]+', record[2].strip()))\n",
    "        _, *Rv = map(int, re.findall('[0-9]+', record[3].strip()))\n",
    "        Lu, Lv = map(int, record[4].split(\" \"))\n",
    "        TWu, TWv = map(int, record[5].split(\" \"))\n",
    "\n",
    "        return Record(op, U, V, delta, Route(Ru, Lu, TWu), Route(Rv, Lv, TWv))\n",
    "\n",
    "    with open(file, 'r') as fh:\n",
    "        args = [iter(fh)] * 6\n",
    "        records = zip(*args)\n",
    "\n",
    "        # This could have been a generator, but each file is only 100-ish MB\n",
    "        # in size, so that comfortably fits in memory.\n",
    "        return [parsed for record in records if (parsed := parse_record(record)).op not in [2, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators (in the order of `main.cpp`):\n",
    "\n",
    "0. $(1, 0)$-Exchange\n",
    "1. $(2, 0)$-Exchange\n",
    "2. $(2, 0)$-Reverse-Exchange\n",
    "3. $(2, 2)$-Exchange\n",
    "4. $(2, 1)$-Exchange\n",
    "5. $(1, 1)$-Exchange\n",
    "6. 2-OPT\n",
    "\n",
    "Note that we currently ignore 2 (reverse exchange) and 6 (two-opt), and focus only on the $(N, M)$-Exchange operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "op2nm = [\n",
    "    (1, 0),\n",
    "    (2, 0),\n",
    "    None,\n",
    "    (2, 2),\n",
    "    (2, 1),\n",
    "    (1, 1),\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(IntEnum):\n",
    "    DIST_PU_V = 0\n",
    "    DIST_PV_U = 1\n",
    "    DIST_VM_UN1 = 2\n",
    "    DIST_UN_VM1 = 3\n",
    "    DIST_PU_U = 4\n",
    "    DIST_PV_V = 5\n",
    "    DIST_UN_UN1 = 6\n",
    "    DIST_VM_VM1 = 7\n",
    "    TW_U_FEAS = 8\n",
    "    TW_V_FEAS = 9\n",
    "    LOAD_U_FEAS = 10\n",
    "    LOAD_V_FEAS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(instance: dict, records: list[Record]) -> np.array:\n",
    "    dist = instance['duration_matrix']\n",
    "    dist_max = dist.max()\n",
    "\n",
    "    data = np.empty((len(records), len(Features)))\n",
    "\n",
    "    for idx, record in enumerate(records):\n",
    "        n, m = op2nm[record.op]\n",
    "\n",
    "        idx_u = record.Ru.index(record.U) if record.U != 0 else -1\n",
    "        idx_v = record.Rv.index(record.V) if record.V != 0 else -1\n",
    "\n",
    "        pu = 0 if idx_u <= 0 else record.Ru[idx_u - 1]\n",
    "        pv = 0 if idx_v <= 0 else record.Rv[idx_v - 1]\n",
    "\n",
    "        dist_un_vm1 = 0\n",
    "        dist_un_un1 = 0\n",
    "        with suppress(IndexError):\n",
    "            un = record.Ru[idx_u + n]\n",
    "            un1 = record.Rv[idx_u + n + 1] if idx_u + n + 1 < len(record.Ru) else 0\n",
    "            dist_un_un1 = dist[un, un1]\n",
    "\n",
    "            vm1 = record.Rv[idx_v + m + 1]   \n",
    "            dist_un_vm1 = dist[un, vm1]\n",
    "\n",
    "        dist_vm_un1 = 0\n",
    "        dist_vm_vm1 = 0\n",
    "        with suppress(IndexError):\n",
    "            vm = record.Rv[idx_v + m]\n",
    "            vm1 = record.Rv[idx_v + m + 1] if idx_v + m + 1 < len(record.Rv) else 0\n",
    "            dist_vm_vm1 = dist[vm, vm1]\n",
    "\n",
    "            un1 = record.Ru[idx_u + n + 1]\n",
    "            dist_vm_un1 = dist[vm, un1]\n",
    "\n",
    "        # Proposed distances\n",
    "        data[idx, Features.DIST_PU_V] = dist[pu, record.V] / dist_max if m > 0 else 0\n",
    "        data[idx, Features.DIST_PV_U] = dist[pv, record.U] / dist_max\n",
    "        data[idx, Features.DIST_VM_UN1] = dist_vm_un1 / dist_max if m > 0 else 0\n",
    "        data[idx, Features.DIST_UN_VM1] = dist_un_vm1 / dist_max\n",
    "\n",
    "        # Existing distances\n",
    "        data[idx, Features.DIST_PU_U] = dist[pu, record.U] / dist_max\n",
    "        data[idx, Features.DIST_PV_V] = dist[pv, record.V] / dist_max\n",
    "        data[idx, Features.DIST_UN_UN1] = dist_un_un1 / dist_max\n",
    "        data[idx, Features.DIST_VM_VM1] = dist_vm_vm1 / dist_max\n",
    "\n",
    "        # Time window feasibility indicators (no time warp indicates feasibility)\n",
    "        data[idx, Features.TW_U_FEAS] = record.Ru.tw == 0\n",
    "        data[idx, Features.TW_V_FEAS] = record.Rv.tw == 0\n",
    "\n",
    "        # Load feasibility (less than vehicle capacity is feasible)\n",
    "        data[idx, Features.LOAD_U_FEAS] = record.Ru.load <= instance['capacity']\n",
    "        data[idx, Features.LOAD_V_FEAS] = record.Rv.load <= instance['capacity']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_retrieve_data(file_loc: str) -> tuple[np.array, np.array]:\n",
    "    cache_loc = DATA_PATH / (Path(file_loc).stem + '.npz')\n",
    "\n",
    "    if cache_loc.exists():\n",
    "        file = np.load(cache_loc)\n",
    "        return file['X'], file['y']                \n",
    "    \n",
    "    instance = tools.read_vrplib(INST_PATH / file_loc)\n",
    "    records = parse_file(DATA_PATH / file_loc)\n",
    "\n",
    "    y = np.array([int(record.delta < 0) for record in records])\n",
    "    X = make_features(instance, records)\n",
    "\n",
    "    np.savez(cache_loc, X=X, y=y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kfold(n_splits: int, weights: dict, files: list[Path]) -> list:\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=4)\n",
    "    fold_results = []\n",
    "\n",
    "    for idx, (train, test) in enumerate(kf.split(files), 1):\n",
    "        print(f\"Fold {idx}\")\n",
    "        model = SGDClassifier(loss=\"log_loss\", class_weight=weights, random_state=0)\n",
    "\n",
    "        for idx in train:\n",
    "            X, y = make_or_retrieve_data(files[idx])\n",
    "            model.partial_fit(X, y, [0, 1])\n",
    "\n",
    "        scores = []\n",
    "        for idx in test:\n",
    "            X, y = make_or_retrieve_data(files[idx])\n",
    "            precision, recall, *_ = score(y, model.predict(X), average='weighted')\n",
    "\n",
    "            # Precision: number of relevant documents retrieved by a search divided by the total number of documents retrieved\n",
    "            # Recall: number of relevant documents retrieved by a search divided by the total number of existing relevant documents\n",
    "            scores.append([precision, recall])\n",
    "\n",
    "        fold_results.append([np.mean(scores, axis=0), model.coef_[0], model.intercept_])\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of improvements appears to be roughly ~0.25% to ~0.3% of the total number of evaluated moves, so we give those a weight of $\\frac{1}{0.003}$ to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 1, 1: 1 / 0.003}\n",
    "files = sorted([Path(file.name) for file in DATA_PATH.glob(\"ORTEC-*.txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n"
     ]
    }
   ],
   "source": [
    "vals = do_kfold(10, weights, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996 \t 0.878 \t [-8.41, -7.77, -0.01, 0.07, 6.11, 8.91, 0.07, 4.27, -1.24, -0.58, -1.34, -0.52] \t 1.79\n",
      "0.996 \t 0.854 \t [-8.41, -7.82, -0.01, 0.02, 6.11, 8.95, 0.1, 4.33, -1.24, -0.58, -1.37, -0.53] \t 1.82\n",
      "0.996 \t 0.863 \t [-8.34, -7.74, 0.04, 0.0, 6.1, 8.91, 0.08, 4.27, -1.24, -0.58, -1.32, -0.52] \t 1.76\n",
      "0.996 \t 0.825 \t [-8.27, -7.65, -0.05, 0.02, 6.1, 8.86, 0.1, 4.38, -1.33, -0.85, -1.19, -0.4] \t 1.94\n",
      "0.996 \t 0.866 \t [-8.5, -7.82, -0.02, 0.01, 6.26, 9.04, 0.12, 4.39, -1.23, -0.58, -1.33, -0.48] \t 1.70\n",
      "0.996 \t 0.881 \t [-8.37, -7.77, -0.02, 0.06, 6.12, 8.9, 0.08, 4.28, -1.24, -0.58, -1.31, -0.51] \t 1.74\n",
      "0.996 \t 0.858 \t [-8.4, -7.85, -0.01, 0.02, 6.14, 8.98, 0.09, 4.33, -1.24, -0.58, -1.34, -0.5] \t 1.75\n",
      "0.996 \t 0.842 \t [-8.4, -7.77, 0.01, 0.03, 6.18, 8.93, 0.08, 4.3, -1.24, -0.58, -1.36, -0.5] \t 1.77\n",
      "0.996 \t 0.871 \t [-8.36, -7.62, -0.08, 0.05, 6.01, 8.86, 0.15, 4.23, -1.24, -0.57, -1.37, -0.51] \t 1.78\n",
      "0.996 \t 0.858 \t [-8.39, -7.79, 0.02, 0.07, 6.06, 8.73, -0.07, 4.1, -1.26, -0.61, -1.33, -0.47] \t 1.82\n"
     ]
    }
   ],
   "source": [
    "for (p, r), coefs, intercept in vals:\n",
    "    print(f\"{p:.3f} \\t {r:.3f} \\t {[round(coef, 2) for coef in coefs]} \\t {intercept[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.37, -7.77, -0.02, 0.06, 6.12, 8.9, 0.08, 4.28, -1.24, -0.58, -1.31, -0.51]\n",
      "1.74\n"
     ]
    }
   ],
   "source": [
    "_, coefs, intercept = max(vals, key=lambda val: val[0][1])  # max recall, since precision is rather static across instances\n",
    "print([round(coef, 2) for coef in coefs])\n",
    "print(round(intercept[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
