{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict bad LS moves for node operators\n",
    "\n",
    "This notebook develops simple models for predicting bad local search moves. Particularly, given nodes $U$ and $V$ in routes $R_U$ and $R_V$, it predicts whether each LS operator we currently have is likely to produce an improving solution if the operator were applied to these node pairs $U$ and $V$.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "  <b>Issue:</b> testing the classifier developed below suggests there is little gain in performance. This is discussed further in issue 65. For something in actual use, have a look at the route-based notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Python\\Euro-NeurIPS-2022\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "from glob import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/raw/\")\n",
    "INST_PATH = Path(\"instances/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "These are used to parse the raw results for a single instance into something that contains the same data, but in a more workable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Route:\n",
    "    clients: list[int]\n",
    "    load: int\n",
    "    tw: int\n",
    "\n",
    "    def index(self, client: int) -> int:\n",
    "        return self.clients.index(client)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> int:\n",
    "        return self.clients[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clients)\n",
    "\n",
    "@dataclass\n",
    "class Record:\n",
    "    op: int\n",
    "    U: int\n",
    "    V: int\n",
    "    delta: int\n",
    "    Ru: Route\n",
    "    Rv: Route\n",
    "\n",
    "def parse_file(file: str) -> list[Record]:\n",
    "    def parse_record(record: list[str]) -> Record:\n",
    "        op = int(record[0].strip())\n",
    "        U, V, delta = map(int, record[1].strip().split(\" \"))\n",
    "        _, *Ru = map(int, re.findall('[0-9]+', record[2].strip()))\n",
    "        _, *Rv = map(int, re.findall('[0-9]+', record[3].strip()))\n",
    "        Lu, Lv = map(int, record[4].split(\" \"))\n",
    "        TWu, TWv = map(int, record[5].split(\" \"))\n",
    "\n",
    "        return Record(op, U, V, delta, Route(Ru, Lu, TWu), Route(Rv, Lv, TWv))\n",
    "\n",
    "    with open(file, 'r') as fh:\n",
    "        args = [iter(fh)] * 6\n",
    "        records = zip(*args)\n",
    "\n",
    "        # This could have been a generator, but each file is only 100-ish MB\n",
    "        # in size, so that comfortably fits in memory. Also, we for now ignore\n",
    "        # the reverse exchange (2) and 2-OPT (6) operators.\n",
    "        return [parsed for record in records \n",
    "                if (parsed := parse_record(record)).op not in [2, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators (in the order of `main.cpp`):\n",
    "\n",
    "0. $(1, 0)$-Exchange\n",
    "1. $(2, 0)$-Exchange\n",
    "2. $(2, 0)$-Reverse-Exchange\n",
    "3. $(2, 2)$-Exchange\n",
    "4. $(2, 1)$-Exchange\n",
    "5. $(1, 1)$-Exchange\n",
    "6. 2-OPT\n",
    "\n",
    "Note that we currently ignore 2 (reverse exchange) and 6 (2-opt), and focus only on the $(N, M)$-Exchange operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "op2nm = [\n",
    "    (1, 0),\n",
    "    (2, 0),\n",
    "    None,\n",
    "    (2, 2),\n",
    "    (2, 1),\n",
    "    (1, 1),\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(IntEnum):\n",
    "    DELTA_DIST_U = 0\n",
    "    DELTA_DIST_V = 1\n",
    "    DELTA_DIST_UN = 2\n",
    "    DELTA_DIST_VM = 3\n",
    "    TW_U_INFEAS = 4\n",
    "    TW_V_INFEAS = 5\n",
    "    LD_U_INFEAS = 6\n",
    "    LD_V_INFEAS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(instance: dict, records: list[Record]) -> np.array:\n",
    "    dist = instance['duration_matrix']\n",
    "    dist_max = dist.max()\n",
    "\n",
    "    data = np.empty((len(records), len(Features)))\n",
    "\n",
    "    for idx, record in enumerate(records):\n",
    "        n, m = op2nm[record.op]\n",
    "\n",
    "        idx_u = record.Ru.index(record.U) if record.U != 0 else -1\n",
    "        idx_v = record.Rv.index(record.V) if record.V != 0 else -1\n",
    "\n",
    "        pu = 0 if idx_u <= 0 else record.Ru[idx_u - 1]\n",
    "        pv = 0 if idx_v <= 0 else record.Rv[idx_v - 1]\n",
    "\n",
    "        dist_un_vm1 = 0\n",
    "        dist_un_un1 = 0\n",
    "        with suppress(IndexError):\n",
    "            un = record.Ru[idx_u + n]\n",
    "            un1 = record.Rv[idx_u + n + 1] if idx_u + n + 1 < len(record.Ru) else 0\n",
    "            dist_un_un1 = dist[un, un1]\n",
    "\n",
    "            vm1 = record.Rv[idx_v + m + 1]   \n",
    "            dist_un_vm1 = dist[un, vm1]\n",
    "\n",
    "        dist_vm_un1 = 0\n",
    "        dist_vm_vm1 = 0\n",
    "        with suppress(IndexError):\n",
    "            vm = record.Rv[idx_v + m]\n",
    "            vm1 = record.Rv[idx_v + m + 1] if idx_v + m + 1 < len(record.Rv) else 0\n",
    "            dist_vm_vm1 = dist[vm, vm1]\n",
    "\n",
    "            un1 = record.Ru[idx_u + n + 1]\n",
    "            dist_vm_un1 = dist[vm, un1]\n",
    "\n",
    "        # Some of these features are unused for pure relocate moves (m == 0)\n",
    "        data[idx, Features.DELTA_DIST_U] = (dist[pv, record.U] if m > 0 else dist[record.V, record.U]) - dist[pu, record.U]\n",
    "        data[idx, Features.DELTA_DIST_V] = dist[pu, record.V] - dist[pv, record.V] if m > 0 else 0\n",
    "        data[idx, Features.DELTA_DIST_UN] = dist_un_vm1 - dist_un_un1\n",
    "        data[idx, Features.DELTA_DIST_VM] = dist_vm_un1 - dist_vm_vm1 if m > 0 else 0\n",
    "\n",
    "        data[idx] /= dist_max  # normalise all distances to [0, 1]\n",
    "\n",
    "        data[idx, Features.TW_U_INFEAS] = record.Ru.tw > 0\n",
    "        data[idx, Features.TW_V_INFEAS] = record.Rv.tw > 0\n",
    "\n",
    "        data[idx, Features.LD_U_INFEAS] = record.Ru.load > instance['capacity']\n",
    "        data[idx, Features.LD_V_INFEAS] = record.Rv.load > instance['capacity']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_retrieve_data(file_loc: str) -> tuple[np.array, np.array]:\n",
    "    cache_loc = DATA_PATH / (Path(file_loc).stem + '.npz')\n",
    "\n",
    "    if cache_loc.exists():\n",
    "        file = np.load(cache_loc)\n",
    "        return file['X'], file['y']                \n",
    "    \n",
    "    instance = tools.read_vrplib(INST_PATH / file_loc)\n",
    "    records = parse_file(DATA_PATH / file_loc)\n",
    "\n",
    "    y = np.array([int(record.delta < 0) for record in records])\n",
    "    X = make_features(instance, records)\n",
    "\n",
    "    np.savez(cache_loc, X=X, y=y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kfold(n_splits: int, weights: dict, files: list[Path]) -> list:\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    fold_results = []\n",
    "\n",
    "    for idx, (train, test) in enumerate(kf.split(files), 1):\n",
    "        print(f\"Fold {idx}\")\n",
    "        model = SGDClassifier(loss=\"log_loss\", \n",
    "                              class_weight=weights, \n",
    "                              random_state=idx)\n",
    "\n",
    "        for idx in train:\n",
    "            X, y = make_or_retrieve_data(files[idx])\n",
    "            model.partial_fit(X, y, [0, 1])\n",
    "\n",
    "        scores = []\n",
    "        for idx in test:\n",
    "            X, y = make_or_retrieve_data(files[idx])\n",
    "            precision, recall, f1score, _ = score(y,\n",
    "                                                  model.predict(X),\n",
    "                                                  average='weighted')\n",
    "\n",
    "            # Precision: number of relevant documents retrieved by a search \n",
    "            #            divided by the total number of documents retrieved\n",
    "            # Recall: number of relevant documents retrieved by a search \n",
    "            #         divided by the total number of existing relevant documents\n",
    "            # F1 score: 2 * (precision * recall) / (precision + recall)\n",
    "            scores.append([precision, recall, f1score])\n",
    "\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        fold_results.append([mean_scores, model.coef_[0], model.intercept_])\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of improvements appears to be roughly ~0.25% to ~0.3% of the total number of evaluated moves, so we give those a weight of $\\frac{1}{0.003}$ to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 1, 1: 1 / 0.003}\n",
    "files = sorted([Path(file.name) for file in DATA_PATH.glob(\"ORTEC-*.txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n"
     ]
    }
   ],
   "source": [
    "vals = do_kfold(10, weights, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folding results: precision, recall, and F1 score. The best folding result (according to F1 score) is marked with a \\*, but the coefficients and performance should all be roughly similar across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FOLD: 1\n",
      "   PREC: 0.996\n",
      " RECALL: 0.816\n",
      "     F1: 0.894\n",
      " COEFFS:  -0.84  -9.41 -11.28  -0.01  -0.92   1.48   0.81   1.57   0.26 \n",
      "\n",
      "   FOLD: 2\n",
      "   PREC: 0.996\n",
      " RECALL: 0.811\n",
      "     F1: 0.891\n",
      " COEFFS:  -0.86  -9.34 -11.18  -0.02  -0.92   1.46   0.73   1.57   0.26 \n",
      "\n",
      "   FOLD: 3\n",
      "   PREC: 0.996\n",
      " RECALL: 0.812\n",
      "     F1: 0.892\n",
      " COEFFS:  -0.85  -9.22 -11.03   0.01  -0.95   1.51   0.78   1.55   0.26 \n",
      "\n",
      "   FOLD: 4\n",
      "   PREC: 0.996\n",
      " RECALL: 0.829\n",
      "     F1: 0.902\n",
      " COEFFS:  -0.94  -9.35 -11.22  -0.04  -0.83   1.49   0.78   1.64   0.27 \n",
      "\n",
      "   FOLD: 5\n",
      "   PREC: 0.997\n",
      " RECALL: 0.769\n",
      "     F1: 0.865\n",
      " COEFFS:  -0.60  -9.11 -11.13  -0.09  -0.97   1.51   0.91   1.41   0.20 \n",
      "\n",
      "   FOLD: 6\n",
      "   PREC: 0.996\n",
      " RECALL: 0.839\n",
      "*    F1: 0.908\n",
      " COEFFS:  -0.89  -9.34 -11.19  -0.01  -0.91   1.49   0.81   1.58   0.25 \n",
      "\n",
      "   FOLD: 7\n",
      "   PREC: 0.996\n",
      " RECALL: 0.810\n",
      "     F1: 0.890\n",
      " COEFFS:  -0.79  -9.08 -10.82   0.04  -0.86   1.54   0.84   1.56   0.29 \n",
      "\n",
      "   FOLD: 8\n",
      "   PREC: 0.996\n",
      " RECALL: 0.831\n",
      "     F1: 0.904\n",
      " COEFFS:  -1.00  -9.39 -11.25  -0.03  -0.94   1.45   0.75   1.57   0.23 \n",
      "\n",
      "   FOLD: 9\n",
      "   PREC: 0.996\n",
      " RECALL: 0.808\n",
      "     F1: 0.890\n",
      " COEFFS:  -0.82  -9.34 -11.17  -0.01  -0.92   1.46   0.81   1.53   0.24 \n",
      "\n",
      "   FOLD: 10\n",
      "   PREC: 0.996\n",
      " RECALL: 0.819\n",
      "     F1: 0.895\n",
      " COEFFS:  -0.90  -9.32 -11.14  -0.03  -0.92   1.39   0.71   1.56   0.24 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_max = max(range(len(vals)), key=lambda idx: vals[idx][0][2])\n",
    "\n",
    "for idx, ((p, r, f1), coefs, intercept) in enumerate(vals, 1):\n",
    "    print(f\"   FOLD: {idx}\")\n",
    "    print(f\"   PREC: {p:.3f}\")\n",
    "    print(f\" RECALL: {r:.3f}\")\n",
    "    print(\"*\" if idx - 1 == idx_max else \" \", f\"   F1: {f1:.3f}\")\n",
    "\n",
    "    coefs = [intercept[0]] + coefs.tolist()\n",
    "    fmt = \"{:6.2f} \" * len(coefs)\n",
    "    print(f\" COEFFS: {fmt.format(*coefs)}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
