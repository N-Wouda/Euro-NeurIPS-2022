{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict bad LS moves\n",
    "\n",
    "This notebook develops simple models for predicting bad local search moves. Particularly, given routes $R_U$ and $R_V$, it predicts whether the route-based LS operators we currently have are likely to produce an improving solution if the operator were to be applied to these route pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Python\\Euro-NeurIPS-2022\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass\n",
    "from enum import IntEnum\n",
    "from glob import glob\n",
    "from functools import cache\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data/raw/\")\n",
    "INST_PATH = Path(\"instances/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "These are used to parse the raw results for a single instance into something that contains the same data, but in a more workable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Route:\n",
    "    clients: list[int]\n",
    "    load: int\n",
    "    tw: int\n",
    "\n",
    "    def index(self, client: int) -> int:\n",
    "        return self.clients.index(client)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> int:\n",
    "        return self.clients[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clients)\n",
    "\n",
    "@dataclass\n",
    "class Record:\n",
    "    op: int\n",
    "    delta: int\n",
    "    Ru: tuple[Route, Optional[Route]]  # tuples of (before, after). After only\n",
    "    Rv: tuple[Route, Optional[Route]]  # when delta is negative.\n",
    "\n",
    "def parse_file(file: str) -> list[Record]:\n",
    "    def parse_record(record: list[str]) -> Record:\n",
    "        op, delta = map(int, record[0].split(\" \"))\n",
    "        _, *Ru = map(int, re.findall('[0-9]+', record[1].strip()))\n",
    "        _, *Rv = map(int, re.findall('[0-9]+', record[2].strip()))\n",
    "        Lu, Lv = map(int, record[3].split(\" \"))\n",
    "        TWu, TWv = map(int, record[4].split(\" \"))\n",
    "\n",
    "        uRouteBefore = Route(Ru, Lu, TWu)  # before\n",
    "        vRouteBefore = Route(Rv, Lv, TWv)  # before\n",
    "        \n",
    "        uRouteAfter = None\n",
    "        vRouteAfter = None\n",
    "        \n",
    "        if delta < 0:\n",
    "            _, *Ru = map(int, re.findall('[0-9]+', record[5].strip()))\n",
    "            _, *Rv = map(int, re.findall('[0-9]+', record[6].strip()))\n",
    "            Lu, Lv = map(int, record[7].split(\" \"))\n",
    "            TWu, TWv = map(int, record[8].split(\" \"))\n",
    "            \n",
    "            uRouteAfter = Route(Ru, Lu, TWu)\n",
    "            vRouteAfter = Route(Rv, Lv, TWv)\n",
    "\n",
    "        return Record(op, \n",
    "                      delta, \n",
    "                      (uRouteBefore, uRouteAfter), \n",
    "                      (vRouteBefore, vRouteAfter))\n",
    "\n",
    "    with open(file, 'r') as fh:\n",
    "        records = []\n",
    "        lines = fh.readlines()\n",
    "        idx = 0\n",
    "\n",
    "        while idx != len(lines):\n",
    "            op, delta = map(int, lines[idx].split(\" \"))\n",
    "\n",
    "            if delta < 0:\n",
    "                record = lines[idx : idx + 9]\n",
    "                records.append(parse_record(record))\n",
    "                idx += 9\n",
    "            else:\n",
    "                record = lines[idx : idx + 5]\n",
    "                records.append(parse_record(record))\n",
    "                idx += 5\n",
    "\n",
    "        return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators (in the order of `main.cpp`):\n",
    "\n",
    "0. RELOCATE*\n",
    "1. SWAP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(IntEnum):\n",
    "    TW_U_INFEAS = 0\n",
    "    TW_V_INFEAS = 1\n",
    "    LD_U_INFEAS = 2\n",
    "    LD_V_INFEAS = 3\n",
    "    U_SIZE = 4\n",
    "    V_SIZE = 5\n",
    "    UV_ANGLE_DIFF = 6\n",
    "    MIN_NODE_DIST = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_angle(coords, route):\n",
    "    if len(route) == 0:\n",
    "        return 0\n",
    "\n",
    "    dx = coords[route.clients][:, 0].mean() - coords[0, 0]\n",
    "    dy = coords[route.clients][:, 1].mean() - coords[0, 1]\n",
    "    \n",
    "    if dy < 0:\n",
    "        return -abs(1 - dx / (abs(dx) + abs(dy)))\n",
    "\n",
    "    return abs(1 - dx / (abs(dx) + abs(dy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(instance: dict, records: list[Record]) -> np.array:\n",
    "    data = np.zeros((len(records), len(Features)))\n",
    "    coords = instance['coords']\n",
    "    dist = instance['duration_matrix']\n",
    "    max_dist = dist.max()\n",
    "\n",
    "    for idx, record in enumerate(records):\n",
    "        uRouteBefore, uRouteAfter = record.Ru\n",
    "        vRouteBefore, vRouteAfter = record.Rv\n",
    "\n",
    "        data[idx, Features.TW_U_INFEAS] = uRouteBefore.tw > 0\n",
    "        data[idx, Features.TW_V_INFEAS] = vRouteBefore.tw > 0\n",
    "\n",
    "        data[idx, Features.LD_U_INFEAS] = uRouteBefore.load > instance['capacity']\n",
    "        data[idx, Features.LD_V_INFEAS] = vRouteBefore.load > instance['capacity']\n",
    "\n",
    "        data[idx, Features.U_SIZE] = len(uRouteBefore) / len(coords)\n",
    "        data[idx, Features.V_SIZE] = len(vRouteBefore) / len(coords)\n",
    "\n",
    "        uAngle = pseudo_angle(coords, uRouteBefore)\n",
    "        vAngle = pseudo_angle(coords, vRouteBefore)\n",
    "\n",
    "        assert -2 <= uAngle <= 2, f\"Got: {uAngle} for {uRouteBefore}\"\n",
    "        assert -2 <= vAngle <= 2, f\"Got: {vAngle} for {vRouteBefore}\"\n",
    "\n",
    "        data[idx, Features.UV_ANGLE_DIFF] = vAngle - uAngle  # in [-4, 4]\n",
    "\n",
    "        min_record_dist = max_dist\n",
    "\n",
    "        for u in uRouteBefore.clients:\n",
    "            for v in vRouteBefore.clients:\n",
    "                min_record_dist = min(dist[u, v], dist[v, u], min_record_dist)\n",
    " \n",
    "        data[idx, Features.MIN_NODE_DIST] = min_record_dist / max_dist\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def make_or_retrieve_data(file_loc: str) -> tuple[np.array, np.array]:\n",
    "    cache_loc = DATA_PATH / (Path(file_loc).stem + '.npz')\n",
    "\n",
    "    if cache_loc.exists():\n",
    "        file = np.load(cache_loc)\n",
    "        return file['X'], file['y']                \n",
    "\n",
    "    instance = tools.read_vrplib(INST_PATH / file_loc)\n",
    "    records = parse_file(DATA_PATH / file_loc)\n",
    "\n",
    "    y = np.array([int(record.delta < 0) for record in records])\n",
    "    X = make_features(instance, records)\n",
    "\n",
    "    np.savez(cache_loc, X=X, y=y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_kfold(n_splits: int, weights: dict, files: list[Path]) -> list:\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(kf.split(files), 1):\n",
    "        print(f\"Fold {fold_idx}\")\n",
    "        model = SGDClassifier(loss=\"log_loss\", \n",
    "                              class_weight=weights, \n",
    "                              random_state=fold_idx)\n",
    "\n",
    "        for idx in train:\n",
    "            X, y = make_or_retrieve_data(files[idx])           \n",
    "            assert y.sum() > 0\n",
    "\n",
    "            model.partial_fit(X, y, [0, 1])\n",
    "\n",
    "        scores = []\n",
    "        for idx in test:\n",
    "            X, y = make_or_retrieve_data(files[idx])\n",
    "            assert y.sum() > 0\n",
    "\n",
    "            precision, recall, f1score, _ = score(y,\n",
    "                                                  model.predict(X),\n",
    "                                                  average='weighted',\n",
    "                                                  zero_division='warn')\n",
    "\n",
    "            # Precision: number of relevant documents retrieved by a search \n",
    "            #            divided by the total number of documents retrieved\n",
    "            # Recall: number of relevant documents retrieved by a search \n",
    "            #         divided by the total number of existing relevant documents\n",
    "            # F1 score: 2 * (precision * recall) / (precision + recall)\n",
    "            scores.append([precision, recall, f1score])\n",
    "\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        fold_results.append([mean_scores, model.coef_[0], model.intercept_])\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of improvements appears to be at least ~0.2% of the total number of evaluated moves, so we give those a weight of $0.002$ to the non-improving label to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 0.005, 1: 1}\n",
    "files = sorted([Path(file.name) for file in DATA_PATH.glob(\"ORTEC-*.txt\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n"
     ]
    }
   ],
   "source": [
    "vals = do_kfold(10, weights, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folding results: precision, recall, and F1 score. The best folding result (according to F1 score) is marked with a \\*, but the coefficients and performance should all be roughly similar across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FOLD: 1\n",
      "   PREC: 0.992\n",
      " RECALL: 0.980\n",
      "*    F1: 0.986\n",
      " COEFFS:  -0.24   0.14   0.12   0.19   0.12  -0.01  -0.03   0.24  -0.85 \n",
      "\n",
      "   FOLD: 2\n",
      "   PREC: 0.991\n",
      " RECALL: 0.979\n",
      "     F1: 0.985\n",
      " COEFFS:  -0.23   0.14   0.11   0.20   0.13  -0.01  -0.03   0.24  -0.86 \n",
      "\n",
      "   FOLD: 3\n",
      "   PREC: 0.992\n",
      " RECALL: 0.971\n",
      "     F1: 0.981\n",
      " COEFFS:  -0.20   0.14   0.12   0.20   0.13  -0.01  -0.03   0.25  -0.85 \n",
      "\n",
      "   FOLD: 4\n",
      "   PREC: 0.991\n",
      " RECALL: 0.977\n",
      "     F1: 0.984\n",
      " COEFFS:  -0.26   0.14   0.13   0.19   0.14  -0.01  -0.03   0.24  -0.85 \n",
      "\n",
      "   FOLD: 5\n",
      "   PREC: 0.992\n",
      " RECALL: 0.974\n",
      "     F1: 0.983\n",
      " COEFFS:  -0.22   0.15   0.14   0.20   0.12  -0.01  -0.02   0.24  -0.82 \n",
      "\n",
      "   FOLD: 6\n",
      "   PREC: 0.991\n",
      " RECALL: 0.972\n",
      "     F1: 0.981\n",
      " COEFFS:  -0.23   0.14   0.13   0.18   0.12  -0.01  -0.03   0.24  -0.83 \n",
      "\n",
      "   FOLD: 7\n",
      "   PREC: 0.992\n",
      " RECALL: 0.971\n",
      "     F1: 0.981\n",
      " COEFFS:  -0.22   0.15   0.12   0.20   0.13  -0.01  -0.02   0.24  -0.84 \n",
      "\n",
      "   FOLD: 8\n",
      "   PREC: 0.992\n",
      " RECALL: 0.975\n",
      "     F1: 0.983\n",
      " COEFFS:  -0.24   0.14   0.12   0.20   0.13  -0.01  -0.03   0.25  -0.84 \n",
      "\n",
      "   FOLD: 9\n",
      "   PREC: 0.991\n",
      " RECALL: 0.973\n",
      "     F1: 0.982\n",
      " COEFFS:  -0.22   0.15   0.11   0.19   0.12  -0.02  -0.03   0.24  -0.85 \n",
      "\n",
      "   FOLD: 10\n",
      "   PREC: 0.992\n",
      " RECALL: 0.976\n",
      "     F1: 0.984\n",
      " COEFFS:  -0.24   0.14   0.12   0.20   0.12  -0.01  -0.03   0.26  -0.84 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_max = max(range(len(vals)), key=lambda idx: vals[idx][0][2])\n",
    "\n",
    "for idx, ((p, r, f1), coefs, intercept) in enumerate(vals, 1):\n",
    "    print(f\"   FOLD: {idx}\")\n",
    "    print(f\"   PREC: {p:.3f}\")\n",
    "    print(f\" RECALL: {r:.3f}\")\n",
    "    print(\"*\" if idx - 1 == idx_max else \" \", f\"   F1: {f1:.3f}\")\n",
    "\n",
    "    coefs = [intercept[0]] + coefs.tolist()\n",
    "    fmt = \"{:6.2f} \" * len(coefs)\n",
    "    print(f\" COEFFS: {fmt.format(*coefs)}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
